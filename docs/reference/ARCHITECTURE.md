# Infrastructure MCP Server - Architecture

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Claude Desktop                            â”‚
â”‚                    (MCP Client / AI Assistant)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ JSON-RPC over stdio
                           â”‚ (MCP Protocol 2025-06-18)
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Infrastructure MCP Server                     â”‚
â”‚                         (infra_mcp)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  StdioMcpServer â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  InfraMcpServer  â”‚              â”‚
â”‚  â”‚  (Transport)    â”‚         â”‚  (Core Logic)    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                             â”‚                         â”‚
â”‚         â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â”‚                     â–¼                â–¼                â”‚
â”‚         â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚         â”‚            â”‚InfraMonitor â”‚  â”‚ LogAnalyzer    â”‚       â”‚
â”‚         â”‚            â”‚(System Data)â”‚  â”‚(AI Analysis)   â”‚       â”‚
â”‚         â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                     â”‚                â”‚                â”‚
â”‚         â”‚                     â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚         â”‚                     â”‚        â”‚  LLM Priority: â”‚       â”‚
â”‚         â”‚                     â”‚        â”‚  1. Ollama âœ…  â”‚       â”‚
â”‚         â”‚                     â”‚        â”‚  2. Gemini     â”‚       â”‚
â”‚         â”‚                     â”‚        â”‚  3. Mock       â”‚       â”‚
â”‚         â”‚                     â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                               â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼               â–¼                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Ollama   â”‚  â”‚ Gemini API â”‚  â”‚  Mock Mode   â”‚
        â”‚  (Local)  â”‚  â”‚ (Cloud)    â”‚  â”‚  (Patterns)  â”‚
        â”‚ PRIMARY âœ…â”‚  â”‚  Fallback  â”‚  â”‚   Fallback   â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚              â”‚
              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      macOS/Linux System                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ CPU, Memory, Disk (psutil)                                   â”‚
â”‚  â€¢ System Logs (/var/log/*)                                     â”‚
â”‚  â€¢ Services (systemctl)                                         â”‚
â”‚  â€¢ Network Interfaces (psutil)                                  â”‚
â”‚  â€¢ User Sessions (psutil)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Architecture

### 1. MCP Server Layer (`server.py`)

**Responsibilities:**
- JSON-RPC request handling
- MCP protocol implementation
- Request routing
- Response formatting
- Error handling

**Key Classes:**
- `InfraMcpServer` - Core MCP logic
- `StdioMcpServer` - stdio transport

**MCP Methods Supported:**
- `initialize` - Server initialization
- `tools/list` - List available tools
- `tools/call` - Execute tools
- `prompts/list` - Empty (not used)
- `resources/list` - Empty (not used)

### 2. Infrastructure Monitor (`infra_monitor.py`)

**Purpose:** Collect system information and metrics

**Key Class:** `InfraMonitor`

**Methods:**
- `get_system_info()` - CPU, memory, disk, uptime
- `get_service_status(service_name)` - Services & processes
- `get_user_info()` - Active sessions & system users
- `get_logs(log_type, lines)` - System log retrieval
- `get_network_info()` - Network interfaces & connections

**Data Sources:**
- `psutil` - System metrics
- `/var/log/*` - Log files
- `systemctl` - Service status
- `/etc/passwd` - User information
- `dmesg` - Kernel messages

### 3. Log Analyzer (`log_analyzer.py`)

**Purpose:** AI-powered log analysis with intelligent LLM fallback

**Key Class:** `LogAnalyzer`

**LLM Integration Strategy:**
1. **Ollama (Primary)** - Local LLM for privacy-first analysis
   - Runs on http://localhost:11434
   - No data leaves your machine
   - No API costs
   - Offline capability
   - Auto-detects available models

2. **Gemini (Fallback)** - Cloud AI when Ollama unavailable
   - Google Generative AI
   - Requires API key
   - Fallback if Ollama fails

3. **Mock Mode (Last Resort)** - Pattern-based analysis
   - No external dependencies
   - Regex pattern matching
   - Always available

**Features:**
- **Multiple Analysis Types:**
  - Summary - Overall assessment
  - Errors - Error detection
  - Security - Security events
  - Performance - Performance insights
- **Automatic LLM Detection:**
  - Checks Ollama availability on startup
  - Discovers installed models
  - Falls back gracefully if unavailable

**Methods:**
- `analyze_logs(logs, analysis_type)` - Log analysis with LLM priority
- `analyze_system_health(system_info, logs)` - Health assessment
- `_call_ollama_api(prompt)` - Ollama API integration
- `_initialize_ollama()` - Ollama detection and configuration

### 4. MCP Types (`mcp_types.py`)

**Purpose:** MCP protocol data structures

**Key Components:**
- Protocol types (Tool, ToolResult, etc.)
- Request/Response structures
- Serialization helpers
- Type definitions

## Data Flow

### Tool Execution Flow

```
1. Claude Desktop Request
   â”‚
   â”œâ”€â–º JSON-RPC over stdin
   â”‚
2. StdioMcpServer receives
   â”‚
   â”œâ”€â–º Parse JSON
   â”œâ”€â–º Extract method & params
   â”‚
3. InfraMcpServer handles
   â”‚
   â”œâ”€â–º Route to handler
   â”œâ”€â–º Validate parameters
   â”‚
4. Execute Tool
   â”‚
   â”œâ”€â–º InfraMonitor (system data)
   â”‚   OR
   â””â”€â–º LogAnalyzer (AI analysis)
       â”‚
       â”œâ”€â–º Mock Mode (local patterns)
       â”‚   OR
       â””â”€â–º Gemini API (real AI)
   â”‚
5. Format Response
   â”‚
   â”œâ”€â–º Create ToolResult
   â”œâ”€â–º Serialize to JSON
   â”‚
6. Return via stdout
   â”‚
   â””â”€â–º JSON-RPC response
       â”‚
7. Claude Desktop receives
   â”‚
   â””â”€â–º Display to user
```

### Log Analysis Flow (with Ollama Priority)

```
User Question
    â”‚
    â–¼
Claude decides to analyze logs
    â”‚
    â”œâ”€â–º Call: get_logs(log_type, lines)
    â”‚   â””â”€â–º Returns: raw log data
    â”‚
    â”œâ”€â–º Call: analyze_logs(log_type, analysis_type)
    â”‚   â”‚
    â”‚   â”œâ”€â–º Get logs from InfraMonitor
    â”‚   â”‚
    â”‚   â””â”€â–º LogAnalyzer.analyze_logs()
    â”‚       â”‚
    â”‚       â”œâ”€â–º PRIORITY 1: Try Ollama (Local LLM) âœ…
    â”‚       â”‚   â”‚
    â”‚       â”‚   â”œâ”€â–º Check: http://localhost:11434/api/tags
    â”‚       â”‚   â”‚
    â”‚       â”‚   â”œâ”€â–º If Available:
    â”‚       â”‚   â”‚   â”œâ”€â–º Prepare prompt
    â”‚       â”‚   â”‚   â”œâ”€â–º POST to /api/generate
    â”‚       â”‚   â”‚   â”œâ”€â–º Stream response
    â”‚       â”‚   â”‚   â””â”€â–º Return AI analysis âœ…
    â”‚       â”‚   â”‚
    â”‚       â”‚   â””â”€â–º If Failed: Continue to Priority 2
    â”‚       â”‚
    â”‚       â”œâ”€â–º PRIORITY 2: Try Gemini (Cloud API)
    â”‚       â”‚   â”‚
    â”‚       â”‚   â”œâ”€â–º Check: GEMINI_API_KEY exists
    â”‚       â”‚   â”‚
    â”‚       â”‚   â”œâ”€â–º If Available:
    â”‚       â”‚   â”‚   â”œâ”€â–º Initialize Gemini client
    â”‚       â”‚   â”‚   â”œâ”€â–º Call Gemini API
    â”‚       â”‚   â”‚   â””â”€â–º Return AI analysis
    â”‚       â”‚   â”‚
    â”‚       â”‚   â””â”€â–º If Failed: Continue to Priority 3
    â”‚       â”‚
    â”‚       â””â”€â–º PRIORITY 3: Mock Mode (Pattern-Based)
    â”‚           â”‚
    â”‚           â”œâ”€â–º Pattern detection (ERROR, WARNING, CRITICAL)
    â”‚           â”œâ”€â–º Count occurrences
    â”‚           â”œâ”€â–º Extract key information
    â”‚           â”œâ”€â–º Generate template insights
    â”‚           â””â”€â–º Return mock analysis
    â”‚
    â””â”€â–º Return analysis to Claude
        â”‚
        â””â”€â–º Claude presents to user
```

**LLM Selection Logic:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Is Ollama running on localhost:11434?  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚
       YES           NO
        â”‚             â”‚
        â–¼             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Ollama  â”‚  â”‚ Try Gemini API â”‚
  â”‚   Mode   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚ âœ… LOCAL â”‚           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                  â”‚             â”‚
                 YES           NO
                  â”‚             â”‚
                  â–¼             â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Gemini  â”‚  â”‚   Mock   â”‚
            â”‚   Mode   â”‚  â”‚   Mode   â”‚
            â”‚  CLOUD   â”‚  â”‚ PATTERN  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Tool Implementation

### Tool Registration

```python
tools = [
    Tool(
        name="get_system_info",
        description="Get system information...",
        inputSchema={...}
    ),
    ...
]
```

### Tool Execution

```python
if tool_name == "get_system_info":
    result = self.infra_monitor.get_system_info()
elif tool_name == "analyze_logs":
    logs = self.infra_monitor.get_logs(...)
    analysis = self.log_analyzer.analyze_logs(logs, ...)
    result = {"log_data": logs, "analysis": analysis}
```

## Dependencies & Libraries

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Python 3.8+ Runtime                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  Core Libraries:                            â”‚
â”‚  â€¢ psutil â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º System metrics      â”‚
â”‚  â€¢ requests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Ollama API calls    â”‚
â”‚  â€¢ json â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Serialization       â”‚
â”‚  â€¢ asyncio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Async handling      â”‚
â”‚  â€¢ subprocess â”€â”€â”€â”€â”€â”€â”€â”€â–º System commands     â”‚
â”‚  â€¢ logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Debug/info          â”‚
â”‚                                             â”‚
â”‚  Optional (LLM Fallback):                   â”‚
â”‚  â€¢ google-generativeai â”€â–º Gemini API       â”‚
â”‚                                             â”‚
â”‚  External Services:                         â”‚
â”‚  â€¢ Ollama (localhost:11434) - Primary LLM   â”‚
â”‚    â””â”€â–º Local, private, no API costs âœ…      â”‚
â”‚  â€¢ Google Gemini API - Fallback LLM        â”‚
â”‚    â””â”€â–º Cloud-based, requires API key       â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LLM Dependencies

| Component | Required | Purpose | Status |
|-----------|----------|---------|--------|
| **Ollama** | Recommended | Local AI analysis | âœ… Primary |
| `requests>=2.32.0` | Yes | Ollama API | âœ… Required |
| **Gemini API** | Optional | Cloud AI fallback | ğŸ”„ Fallback |
| `google-generativeai>=0.8.3` | Optional | Gemini client | ğŸ”„ Optional |
| **Mock Mode** | Always | Pattern analysis | âœ… Built-in |

## Configuration

### Claude Desktop Configuration

```json
{
  "mcpServers": {
    "infra-monitor": {
      "command": "/path/to/python3",
      "args": ["-u", "-m", "infra_mcp.server"],
      "cwd": "/path/to/InfraGPT-NerdMeetup",
      "env": {
        "PYTHONUNBUFFERED": "1",
        "OLLAMA_URL": "http://localhost:11434",
        "OLLAMA_MODEL": "llama3.2",
        "GEMINI_API_KEY": "${GEMINI_API_KEY}"
      }
    }
  }
}
```

**Key Configuration Notes:**
- `-u` flag ensures unbuffered Python output (critical for stdio)
- `PYTHONUNBUFFERED=1` enforces unbuffered mode
- Full path to Python interpreter recommended (e.g., from `pyenv`)

### Environment Variables

**Ollama Configuration (Primary LLM):**
- `OLLAMA_URL` - Ollama API endpoint (default: http://localhost:11434)
- `OLLAMA_MODEL` - Preferred model name (default: llama3.2)
  - Options: llama3.2, llama3.1, mistral, codellama, etc.
  - Falls back to any available model if not found

**Gemini Configuration (Fallback LLM):**
- `GEMINI_API_KEY` - Google Gemini API key (optional)
  - Only used if Ollama is unavailable
  - Get key from: https://makersuite.google.com/app/apikey

**General Settings:**
- `PYTHONUNBUFFERED` - Unbuffered output (required: "1")
- `LOG_LEVEL` - Logging level (default: INFO)
- `PYTHONPATH` - Python module path (optional)

**LLM Priority:**
```
Ollama (local) â†’ Gemini (cloud) â†’ Mock (patterns)
     â†“               â†“                 â†“
  Primary         Fallback         Always Available
  No cost         API cost          Free
  Private         Cloud             Local
```

## Security Architecture

### Access Control

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Security Layers                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  1. Process Isolation                       â”‚
â”‚     â””â”€â–º Runs as user process                â”‚
â”‚                                             â”‚
â”‚  2. Permission Handling                     â”‚
â”‚     â””â”€â–º Graceful fallback                   â”‚
â”‚                                             â”‚
â”‚  3. Data Privacy (Enhanced with Ollama) âœ…  â”‚
â”‚     â”œâ”€â–º Ollama: 100% local processing       â”‚
â”‚     â”‚   â””â”€â–º No data leaves your machine     â”‚
â”‚     â”œâ”€â–º Gemini: Optional cloud fallback     â”‚
â”‚     â”‚   â””â”€â–º Only if user provides API key   â”‚
â”‚     â””â”€â–º Mock: Local pattern matching        â”‚
â”‚                                             â”‚
â”‚  4. No Data Storage                         â”‚
â”‚     â””â”€â–º Ephemeral processing                â”‚
â”‚     â””â”€â–º Logs analyzed in memory only        â”‚
â”‚                                             â”‚
â”‚  5. Network Security                        â”‚
â”‚     â”œâ”€â–º Ollama: localhost only              â”‚
â”‚     â”œâ”€â–º Gemini: HTTPS to Google             â”‚
â”‚     â””â”€â–º Mock: No network required           â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Privacy Model with Ollama

```
Data Flow Analysis:

With Ollama (Primary):
  User System â†’ MCP Server â†’ Ollama (localhost:11434)
                                    â†“
                              Local LLM Processing
                                    â†“
                            â† AI Analysis Returns
  âœ… Data never leaves machine
  âœ… Complete privacy
  âœ… No API logging
  âœ… Works offline

With Gemini (Fallback):
  User System â†’ MCP Server â†’ Internet â†’ Google Gemini API
                                              â†“
                                        Cloud Processing
                                              â†“
                            â† AI Analysis Returns
  âš ï¸ Data sent to cloud
  âš ï¸ Subject to Google's privacy policy
  âœ… Encrypted in transit (HTTPS)

With Mock (Last Resort):
  User System â†’ MCP Server â†’ Pattern Matching (local)
                                    â†“
                            â† Analysis Returns
  âœ… Completely local
  âœ… Zero network access
  âš ï¸ Basic analysis only
```

### Permission Model

```
System Resource          Permission    Fallback
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CPU/Memory/Disk          User          None
Running Processes        User          Limited list
System Logs              Read          tail command
Service Status           User/sudo     Partial
Network Info             User          Limited
User Sessions            User          Current only
```

## Performance Characteristics

### Resource Usage

- **Memory**: ~50-100 MB (MCP server idle)
- **CPU**: <1% (idle), 5-15% (during analysis)
- **Startup**: ~1-2 seconds
- **Response Time**: 
  - System info: <1 second
  - Log retrieval: 1-2 seconds
  - AI analysis: 
    - **Ollama**: 3-8 seconds (local inference) âœ…
    - **Gemini**: 2-5 seconds (API latency)
    - **Mock**: <1 second (pattern matching)

### LLM Performance Comparison

| Metric | Ollama (Local) | Gemini (Cloud) | Mock Mode |
|--------|----------------|----------------|-----------|
| **Latency** | 3-8 seconds | 2-5 seconds | <1 second |
| **Privacy** | 100% local âœ… | Data sent to cloud | 100% local âœ… |
| **Cost** | Free âœ… | API costs | Free âœ… |
| **Offline** | Yes âœ… | No | Yes âœ… |
| **Quality** | High âœ… | Very High | Basic |
| **Setup** | Install Ollama | API key only | None âœ… |

**Ollama Resource Impact:**
- Runs separately as a service
- Memory: ~2-4 GB (depends on model)
- CPU: 20-60% during inference (brief)
- GPU: Optional (significantly faster with GPU)

### Scalability

- **Concurrent Requests**: Handles one at a time (stdio)
- **Log Size**: Configured limits (1-1000 lines)
- **Process List**: Limited to top 10 by CPU

## Error Handling

```
Error Type              Handler             User Impact
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Permission Denied       Try alternatives    Partial data
File Not Found          Return empty        No data
API Timeout             Mock fallback       Mock analysis
Invalid JSON            Parse error         Error message
Missing Tool            Error response      Tool not found
System Error            Log & return        Error message
```

## Extension Points

### Adding New Tools

1. Define tool in `_initialize_tools()`
2. Add handler in `_handle_tool_call()`
3. Implement logic in `InfraMonitor` or `LogAnalyzer`
4. Update documentation

### Adding Analysis Types

1. Add to `analyze_logs()` prompts
2. Create mock analysis template
3. Update tool schema enum
4. Test with `test_client.py`

## Testing Architecture

```
Test Client (test_client.py)
    â”‚
    â”œâ”€â–º Initialize Server
    â”œâ”€â–º List Tools
    â”œâ”€â–º Test Each Tool
    â”‚   â”œâ”€â–º get_system_info
    â”‚   â”œâ”€â–º get_service_status
    â”‚   â”œâ”€â–º get_user_info
    â”‚   â”œâ”€â–º get_logs
    â”‚   â”œâ”€â–º get_network_info
    â”‚   â”œâ”€â–º analyze_logs
    â”‚   â””â”€â–º health_check
    â”‚
    â””â”€â–º Verify Responses
```

## Deployment Model

```
Development:
  Local Python â”€â–º Direct execution â”€â–º Test client

Production (with Ollama - Recommended):
  Ollama Service (localhost:11434)
        â†“
  Claude Desktop â”€â–º stdio MCP â”€â–º InfraGPT-MCP â”€â–º Ollama
                                     â”‚
                                     â””â”€â–º System Monitoring
  âœ… 100% Local
  âœ… Private
  âœ… No API costs

Production (Cloud Fallback):
  Claude Desktop â”€â–º stdio MCP â”€â–º InfraGPT-MCP â”€â–º Gemini API
                                     â”‚
                                     â””â”€â–º System Monitoring
  âš ï¸ Data sent to cloud
  âš ï¸ Requires API key
  âš ï¸ API costs

Alternative:
  Any MCP Client â”€â–º stdio MCP â”€â–º InfraGPT-MCP
```

## Ollama Setup Architecture

### Installation Flow

```
1. Install Ollama
   brew install ollama
   (or download from ollama.ai)
        â†“
2. Start Ollama Service
   ollama serve
   (runs on localhost:11434)
        â†“
3. Pull LLM Model
   ollama pull llama3.2
   (or llama3.1, mistral, etc.)
        â†“
4. Configure MCP Server
   Set OLLAMA_URL=http://localhost:11434
   Set OLLAMA_MODEL=llama3.2 (optional)
        â†“
5. Start Claude Desktop
   MCP server auto-detects Ollama
        â†“
6. Ready! 
   AI analysis now 100% local âœ…
```

### Ollama Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Ollama Service                  â”‚
â”‚    (Background Process)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  HTTP Server: localhost:11434           â”‚
â”‚  â”‚                                      â”‚
â”‚  â”œâ”€â–º /api/tags      (list models)      â”‚
â”‚  â”œâ”€â–º /api/generate  (inference)        â”‚
â”‚  â””â”€â–º /api/chat      (chat interface)   â”‚
â”‚                                         â”‚
â”‚  Model Storage: ~/.ollama/models       â”‚
â”‚  â”‚                                      â”‚
â”‚  â”œâ”€â–º llama3.2  (2GB)                   â”‚
â”‚  â”œâ”€â–º llama3.1  (4.7GB)                 â”‚
â”‚  â””â”€â–º mistral   (4.1GB)                 â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## LLM Selection & Fallback Architecture

### Decision Flow

```python
# Pseudocode for LLM selection
class LogAnalyzer:
    def __init__(self):
        # Priority 1: Try Ollama
        if self._initialize_ollama():
            self.llm_mode = "ollama"
            self.ollama_url = "http://localhost:11434"
            self.ollama_model = self._detect_model()
        
        # Priority 2: Try Gemini
        elif os.getenv("GEMINI_API_KEY"):
            self.llm_mode = "gemini"
            import google.generativeai as genai
            genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        
        # Priority 3: Mock Mode
        else:
            self.llm_mode = "mock"
    
    def analyze_logs(self, logs, analysis_type):
        # Try Ollama first
        if self.llm_mode == "ollama":
            try:
                return self._call_ollama_api(prompt)
            except Exception:
                # Fall back to Gemini
                if os.getenv("GEMINI_API_KEY"):
                    return self._call_gemini_api(prompt)
                # Fall back to Mock
                return self._mock_analysis(logs, analysis_type)
        
        # Try Gemini second
        elif self.llm_mode == "gemini":
            try:
                return self._call_gemini_api(prompt)
            except Exception:
                # Fall back to Mock
                return self._mock_analysis(logs, analysis_type)
        
        # Mock mode (always works)
        else:
            return self._mock_analysis(logs, analysis_type)
```

### Runtime Detection

```
Server Startup:
    â”‚
    â”œâ”€â–º Check Ollama availability
    â”‚   â””â”€â–º GET http://localhost:11434/api/tags
    â”‚       â”‚
    â”‚       â”œâ”€â–º Success: Set llm_mode = "ollama" âœ…
    â”‚       â”‚   â””â”€â–º Detect models: ["llama3.2", "llama3.1", ...]
    â”‚       â”‚
    â”‚       â””â”€â–º Failure: Continue to next check
    â”‚
    â”œâ”€â–º Check Gemini API key
    â”‚   â””â”€â–º Check GEMINI_API_KEY environment variable
    â”‚       â”‚
    â”‚       â”œâ”€â–º Exists: Set llm_mode = "gemini"
    â”‚       â”‚
    â”‚       â””â”€â–º Missing: Continue to next check
    â”‚
    â””â”€â–º Default to Mock Mode
        â””â”€â–º Set llm_mode = "mock" (always available)

Every Analysis Request:
    â”‚
    â”œâ”€â–º Try current llm_mode
    â”‚   â”‚
    â”‚   â”œâ”€â–º Success: Return analysis âœ…
    â”‚   â”‚
    â”‚   â””â”€â–º Failure: Try next fallback
    â”‚       â”‚
    â”‚       â””â”€â–º Eventually reaches Mock Mode (never fails)
```

### LLM Feature Comparison

| Feature | Ollama | Gemini | Mock |
|---------|--------|--------|------|
| **Privacy** | 100% local | Cloud-based | 100% local |
| **Cost** | Free | Pay-per-use | Free |
| **Setup** | Install service | API key only | Built-in |
| **Offline** | Yes âœ… | No | Yes âœ… |
| **Speed** | 3-8s | 2-5s | <1s |
| **Quality** | High | Very High | Basic |
| **Models** | Many options | Fixed | N/A |
| **Customizable** | Yes âœ… | Limited | No |
| **Data retention** | None âœ… | Per Google policy | None âœ… |
| **Network required** | No âœ… | Yes | No âœ… |

### Recommended Configuration

**For Production (Privacy-First):**
```bash
# Best: Ollama only
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
# No GEMINI_API_KEY = stays local always âœ…
```

**For Development (Maximum Reliability):**
```bash
# Ollama primary, Gemini fallback
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
GEMINI_API_KEY=your_api_key_here
```

**For Testing (No Dependencies):**
```bash
# Mock mode only
# No environment variables needed
# Always works, basic analysis
```

## Version Compatibility

| Component | Version | Status |
|-----------|---------|--------|
| MCP Protocol | 2025-06-18 | âœ… Supported |
| Python | 3.8+ | âœ… Required |
| Claude Desktop | Latest | âœ… Tested |
| Ollama | Latest | âœ… Recommended |
| macOS | 11+ | âœ… Tested |
| Linux | Most distros | âœ… Expected |

### LLM Compatibility

| LLM | Integration | Models Tested | Status |
|-----|-------------|---------------|--------|
| **Ollama** | HTTP API | llama3.2, llama3.1, mistral | âœ… Primary |
| **Gemini** | Python SDK | gemini-1.5-flash | âœ… Fallback |
| **Mock** | Built-in | Pattern matching | âœ… Always Available |

---

**Architecture Version**: 2.0 (Ollama Integration)  
**Last Updated**: November 22, 2025  
**Status**: Production Ready âœ…

**Major Changes in v2.0:**
- âœ… Ollama as primary LLM (100% local, private)
- âœ… Intelligent LLM fallback system (Ollama â†’ Gemini â†’ Mock)
- âœ… Enhanced privacy with local-first approach
- âœ… Zero API costs with Ollama
- âœ… Offline capability with Ollama
- âœ… Protocol version updated to 2025-06-18
- âœ… Comprehensive regression testing (33 tests, 100% pass rate)

